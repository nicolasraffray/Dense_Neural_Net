## Current Status

Not working and trying to build functionns outside for loop and then put them in once i know they work. As of now tho everything in the for loop is trash. Like there will be no "best weights". When implemented properly the weights generated by the net will be the best weights.  

Since you last saw I've deaded out and deleted a lot of code form inside the for loop of the run_nn function.  

Also this series is amazing highly reccomend 
https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi

Also sorry for the long ass readme. 

## DATA 

Generating Datasets: https://machinelearningmastery.com/generate-test-datasets-python-scikit-learn/

So the data is just some function for plotting non-linear data I found online. There are other functions you can try out if you look for them on medium too. Definitely take a look before jumping into the code and play around with changing no of "classes or y variable". 

run in terminal python -i simple_net.py

``` python 
X, y = create_data2(100,3)
plot_train(X,y)
```

Note rn I'm net is training on 100% of the dataset. What i think we should do next if this fits the set is use the first create_data function and split it into training set and test set and see how it does with different ratios of training data against test data

## Network

Purpose: Its trying to classify points on a graph correctly

The classes in the code each represent a component of the network. Note in softmax exp_values are made to be negative just as form of normalization. I'm not sure its neccessary i put it (I think youd still get correct output without but values in calculating probs won't be as large). The way the net is now the normalization doesn't matter too much but if you wanted to experiment with adding it to a layer thats not the final layer I think it would be better with it like this?

I used ReLU just bc its easy and commonly used but could try with sigmoid which used to and still is quite popular. 

Categorical cross entropy loss is, I think, the most used cost function for classification w/ softmax. 

I found this really helpful: https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation

So this brings me to the current problem. Which in explaining ot you yesterday I think I clocked. You'll notice, when you run whats under the get visability comment that the mattrix dimensions are just so so wrong. If you look at line 152, I'm computing the derivative of the Cost function wrt the softmax:

where I found the derivative :p :  https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function

The error is clear, true_y is the labels of the classes, i.e. 0,1,2 and not the true predicition. For example if true_y says its class 2 it should show an array of the correct probabilities [0,1,0]. In the case of class 1 [1,0,0], you get the idea. Im not 100% sure about the back prop. From what I've read its just upstream grad * local grad where upstream grad in first step of backprop is dL/dL = 1. 

This resource is great: https://www.kdnuggets.com/2019/08/numpy-neural-networks-computational-graphs.html

But im not sure I've gotten the backprop right.  

## Updating weights

Is easy just W1 -= learningrate*dW1
Same for all weights and biases. 

## Running net 

Will try running to see what the error is doing if good and get low error ie <10% 

We can then after run it forward to get predictions and parameters and threshold at some value like 0.5 for 50% and maybe look into plotting out the its 'decisions' 

